{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilmpXQZHvzFH"
      },
      "source": [
        "## Library Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEIEqiOdfA_3"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from subprocess import check_output\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riMK50i_qgi9"
      },
      "source": [
        "!pip install sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOb0Jps6qj-B"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8onz3TnaGW2C"
      },
      "source": [
        "#augment the stopwords with nonstandard twitter words\n",
        "stopwords_set = set(stopwords.words(\"indonesian\"))\n",
        "stopwords_aug = {\"ya\",\"yak\",\"iya\",\"yg\",\"ga\",\"gak\",\"gk\",\"udh\",\"sdh\",\"udah\",\"dah\",\"nih\",\"ini\",\"deh\",\"sih\",\"dong\",\"donk\",\n",
        "                 \"sm\",\"knp\",\"utk\",\"yaa\",\"tdk\",\"gini\",\"gitu\",\"bgt\",\"gt\",\"nya\",\"kalo\",\"cb\",\"jg\",\"jgn\",\"gw\",\"ge\",\n",
        "                 \"sy\",\"min\",\"mas\",\"mba\",\"mbak\",\"pak\",\"kak\",\"trus\",\"trs\",\"bs\",\"bisa\",\"aja\",\"saja\",\"no\",\n",
        "                 \"w\",\"g\",\"gua\",\"gue\",\"emang\",\"emg\",\"wkwk\",\"dr\",\"kau\",\"dg\",\"gimana\",\"apapun\",\"apa\",\n",
        "                 \"klo\",\"yah\",\"banget\",\"pake\",\"terus\",\"krn\",\"jadi\",\"jd\",\"mu\",\"ku\",\"si\",\"hehe\",\n",
        "                 \"tp\",\"pa\",\"lu\",\"lo\",\"lw\",\"tw\",\"tau\",\"karna\",\"kayak\",\"ky\",\"lg\",\"untuk\",\"tuk\",\"dg\",\"dgn\"}\n",
        "stopwords_all = stopwords_set.union(stopwords_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P9Qjvono_nV"
      },
      "source": [
        "def clean_text(text):\n",
        "    filtered_tokens = \"\"\n",
        "    for token in text:\n",
        "      if re.search('[a-zA-Z\\s]', token):\n",
        "        filtered_tokens = filtered_tokens + token.lower()\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def tokenize_clean(text):\n",
        "\n",
        "    #tokenisasi\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word\n",
        "        in nltk.word_tokenize(sent)]\n",
        "\n",
        "    #clean token from numeric and other character like puntuation\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def remove_stopwords(tokenized_text):\n",
        "\n",
        "    cleaned_token = []\n",
        "    for token in tokenized_text:\n",
        "        if token not in stopwords_all:\n",
        "            cleaned_token.append(token)\n",
        "\n",
        "    return cleaned_token\n",
        "\n",
        "def stemming_text(tokenized_text):\n",
        "\n",
        "    #stem using Sastrawi StemmerFactory\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "\n",
        "    stems = []\n",
        "    for token in tokenized_text:\n",
        "        stems.append(stemmer.stem(token))\n",
        "\n",
        "    return stems\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    #tokenize, remove non alpha numeric and make lower\n",
        "    #text_tmp = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    #text_tmp = text_tmp.lower()\n",
        "    #text_tmp = text_tmp.split()\n",
        "\n",
        "    prep01 = tokenize_clean(text)\n",
        "\n",
        "    #remove stopwords\n",
        "    prep02 = remove_stopwords(prep01)\n",
        "\n",
        "    #stemmingnya lambat banget\n",
        "    #prep03 = stemming_text(prep01)\n",
        "\n",
        "    prep03 = ' '.join(prep02)\n",
        "\n",
        "    return prep03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNBfQVepGWoQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvM7_X5Ivtbd"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icVVtmrxfk5s"
      },
      "source": [
        "!mkdir -p dataset\n",
        "!wget https://raw.githubusercontent.com/project303/dataset/master/Twitter.csv -P dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_6xbIApfqu8"
      },
      "source": [
        "dataset = pd.read_csv('dataset/Twitter.csv', sep='|')\n",
        "\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlSwgdVUGzt3"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfmHJ7z8G29K"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGuzoHjdwEv3"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRPXPYnNG6Dm"
      },
      "source": [
        "dataset = dataset[dataset.sentiment != \"Netral\"]\n",
        "tweets = np.array(dataset['text'])\n",
        "sentiments = np.array(dataset['sentiment'])\n",
        "\n",
        "train_data, test_data, train_label, test_label = train_test_split(tweets, sentiments, test_size=0.2, random_state=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXMSdZYoH95A"
      },
      "source": [
        "train_label[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JalHwtH4hZgc"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7g2ggTeOiXZ"
      },
      "source": [
        "train_data_clean = []\n",
        "\n",
        "for tweet_text in train_data:\n",
        "  train_data_clean.append(text_preprocessing(tweet_text))\n",
        "\n",
        "test_data_clean = []\n",
        "for tweet_text in test_data:\n",
        "  test_data_clean.append(text_preprocessing(tweet_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z61IwycwIqd7"
      },
      "source": [
        "train_data_clean[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbKvjddJwlrc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R1kMeNGwl_g"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV1USBuXIwDV"
      },
      "source": [
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer( ngram_range=(1,1),\n",
        "                     analyzer='word',\n",
        "                     lowercase=True,\n",
        "                     sublinear_tf=True,\n",
        "                     use_idf=True\n",
        "                     )\n",
        "tv_train_features = tv.fit_transform(train_data_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VJnbHF3P1d1"
      },
      "source": [
        "len(tv.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz49NaQbPrMg"
      },
      "source": [
        "tv_train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI4SG8lVahBP"
      },
      "source": [
        "tv_test_features = tv.transform(test_data_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYBsal1NateB"
      },
      "source": [
        "tv_test_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhnenZX2at1k"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeeRH5Hmwz0j"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLdP8aE_bTHV"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "model_lr = LogisticRegression(penalty='l2', max_iter=1000, C=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COSvppmUb3X7"
      },
      "source": [
        "# build model\n",
        "model_lr.fit(tv_train_features, train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IppoG1CctgX"
      },
      "source": [
        "predictions_lr = model_lr.predict(tv_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dstgUIivhevH"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "neg_cnt = 0\n",
        "pos_cnt = 0\n",
        "neg_cnt_x = 0\n",
        "pos_cnt_x = 0\n",
        "\n",
        "for i in range(0, len(test_label)):\n",
        "  if test_label[i] == 'Positif':\n",
        "    pos_cnt = pos_cnt + 1\n",
        "    if test_label[i] == predictions_lr[i]:\n",
        "      pos_cnt_x = pos_cnt_x + 1\n",
        "  else:\n",
        "    neg_cnt = neg_cnt + 1\n",
        "    if test_label[i] == predictions_lr[i]:\n",
        "      neg_cnt_x = neg_cnt_x + 1\n",
        "\n",
        "print(neg_cnt)\n",
        "print(pos_cnt)\n",
        "\n",
        "print('[Positif]: %s/%s '  % (pos_cnt,pos_cnt_x))\n",
        "print('[Negatif]: %s/%s '  % (neg_cnt,neg_cnt_x))\n",
        "\n",
        "print(\"Accuracy(in %):\", metrics.accuracy_score(test_label, predictions_lr)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w06gSH4om9-A"
      },
      "source": [
        "print('Accuracy \\t: ', np.round( metrics.accuracy_score(test_label, predictions_lr), 4))\n",
        "print('Precision \\t: ', np.round(metrics.precision_score(test_label,\n",
        "                                                     predictions_lr,\n",
        "                                                     average='weighted'), 4))\n",
        "print('Recall  \\t: ', np.round( metrics.recall_score(test_label,\n",
        "                                                    predictions_lr,\n",
        "                                                    average='weighted'), 4))\n",
        "print('F1 Score  \\t: ', np.round( metrics.f1_score(test_label,\n",
        "                                                  predictions_lr,\n",
        "                                                  average='weighted'), 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGTuLNiddII"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfBqrQyyaY4I"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(tv_train_features, train_label)\n",
        "predict_nb = model_nb.predict(tv_test_features)\n",
        "print(\"Accuracy(in %):\", metrics.accuracy_score(test_label, predict_nb)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l681WDbRqpSu"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "model_svm = SVC(kernel = 'linear', random_state = 0)\n",
        "model_svm.fit(tv_train_features, train_label)\n",
        "predict_svm = model_svm.predict(tv_test_features)\n",
        "print(\"Accuracy(in %):\", metrics.accuracy_score(test_label, predict_svm)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqtYFXQ3oXPo"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_rf = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n",
        "model_rf.fit(tv_train_features, train_label)\n",
        "predict_rf = model_rf.predict(tv_test_features)\n",
        "print(\"Accuracy(in %):\", metrics.accuracy_score(test_label, predict_rf)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOUw1_g_qx2i"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5XlixC22Dis"
      },
      "source": [
        "## Prediction Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ48D_Oa5BeE"
      },
      "source": [
        "def Predict_Sentiment(text, model):\n",
        "  data_txt =[]\n",
        "  data_txt.append(text_preprocessing(text))\n",
        "  feature_p = tv.transform(data_txt)\n",
        "  predict_p = model.predict(feature_p)\n",
        "\n",
        "  return predict_p[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsam9OWM1u2p"
      },
      "source": [
        "Predict_Sentiment('Telkomsel sinyal jelek', model_svm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}